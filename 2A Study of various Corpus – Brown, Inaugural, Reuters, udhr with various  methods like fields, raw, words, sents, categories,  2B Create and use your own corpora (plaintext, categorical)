a. Study of various Corpus – Brown, Inaugural, Reuters, udhr with various 
methods like fields, raw, words, sents, categories, 
source code:
'''NLTK includes a small selection of texts from the Project brown electronic text 
archive, which contains some 25,000 free electronic books, hosted at 
http://www.brown.org/. We begin by getting the Python interpreter to load the NLTK 
package, then ask to see nltk.corpus.brown.fileids(), the file identifiers in this corpus:'''
import nltk
from nltk.corpus import brown
print ('File ids of brown corpus\n',brown.fileids())
'''Let’s pick out the first of these texts — Emma by Jane Austen — and give it a short 
name, emma, then find out how many words it contains:'''
ca01 = brown.words('ca01')
# display first few words
print('\nca01 has following words:\n',ca01)
# total number of words in ca01
print('\nca01 has',len(ca01),'words')
#categories or files 
print ('\n\nCategories or file in brown corpus:\n')
print (brown.categories())
'''display other information about each text, by looping over all the values of fileid 
corresponding to the brown file identifiers listed earlier and then computing statistics 
for each text.'''
print ('\n\nStatistics for each text:\n')
print 
('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\t\tFileName')
for fileid in brown.fileids():
 num_chars = len(brown.raw(fileid))
 num_words = len(brown.words(fileid))
 num_sents = len(brown.sents(fileid))
 num_vocab = len(set([w.lower() for w in brown.words(fileid)]))
print (int(num_chars/num_words),'\t\t\t', int(num_words/num_sents),'\t\t\t', 
int(num_words/num_vocab),'\t\t\t', fileid)


2b
b. Create and use your own corpora (plaintext, categorical)
'''NLTK includes a small selection of texts from the Project filelist electronic text 
archive, which contains some 25,000 free electronic books, hosted at 
http://www.filelist.org/. We begin by getting the Python interpreter to load the NLTK 
package, then ask to see nltk.corpus.filelist.fileids(), the file identifiers in this corpus:'''
import nltk
from nltk.corpus import PlaintextCorpusReader
corpus_root = 'D:/2020/NLP/Practical/uni'
filelist = PlaintextCorpusReader(corpus_root, '.*')
print ('\n File list: \n')
print (filelist.fileids())
print (filelist.root)
 
'''display other information about each text, by looping over all the values of fileid 
corresponding to the filelist file identifiers listed earlier and then computing statistics 
for each text.'''

print ('\n\nStatistics for each text:\n')
print 
('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\tFileName')
for fileid in filelist.fileids():
 num_chars = len(filelist.raw(fileid))
 num_words = len(filelist.words(fileid))
 num_sents = len(filelist.sents(fileid))
 num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))
 print (int(num_chars/num_words),'\t\t\t', int(num_words/num_sents),'\t\t\t', 
int(num_words/num_vocab),'\t\t', fileid)


